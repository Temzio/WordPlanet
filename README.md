# WordPlanet
WordPlanet is a collection of pre-trained Word2Vec models for natural language processing tasks and research.
## Available Models

| Model name             | Corpus             | Vocabulary size     | Dimensions | Architecture | Window size | Training algorithm | File size   |
|------------------------|--------------------|----------------------|------------|---------------|--------------|---------------------|-------------|
| [WikiPersian‑105M‑v0.1](https://d1.temzio.ir/wordplanet-wikipersian-105M-v0.1.zip) | [Persian Wikipedia](https://github.com/miladfa7/Persian-Wikipedia-Dataset)  | ~105 million tokens | 100          | Word2Vec      | 5            | CBOW    | ~206 MB ZIP |
